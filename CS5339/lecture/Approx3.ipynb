{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Examples for Approx3 Lecture\n",
    "\n",
    "## SVM and Sparsity\n",
    "\n",
    "The support vector formulation provides sparse solutions, in the sense that the weight vector is a linear combination of a small number of instances -- in the case of Hard SVM these are the instances that are at a distance $1/||w||$ from the hyperplane.\n",
    "\n",
    "Soft SVM also has sparse support as shown in the example below. Example taken from http://scikit-learn.org/stable/auto_examples/svm/plot_svm_margin.html. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(__doc__)\n",
    "\n",
    "\n",
    "# Code source: GaÃ«l Varoquaux\n",
    "# Modified for documentation by Jaques Grobler\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "\n",
    "# we create 40 separable points\n",
    "np.random.seed(0)\n",
    "X = np.r_[np.random.randn(20, 2) - [2, 2], np.random.randn(20, 2) + [2, 2]]\n",
    "Y = [0] * 20 + [1] * 20\n",
    "\n",
    "# figure number\n",
    "fignum = 1\n",
    "\n",
    "# fit the model\n",
    "for name, penalty in (('reg1', 1), ('reg2', 0.05)):\n",
    "\n",
    "    clf = svm.SVC(kernel='linear', C=penalty)\n",
    "    clf.fit(X, Y)\n",
    "\n",
    "    # get the separating hyperplane\n",
    "    w = clf.coef_[0]\n",
    "    a = -w[0] / w[1]\n",
    "    xx = np.linspace(-5, 5)\n",
    "    yy = a * xx - (clf.intercept_[0]) / w[1]\n",
    "\n",
    "    # plot the parallels to the separating hyperplane that pass through the\n",
    "    # support vectors\n",
    "    margin = 1 / np.sqrt(np.sum(clf.coef_ ** 2))\n",
    "    yy_down = yy + a * margin\n",
    "    yy_up = yy - a * margin\n",
    "\n",
    "    # plot the line, the points, and the nearest vectors to the plane\n",
    "    plt.figure(fignum, figsize=(6, 4.5))\n",
    "    plt.clf()\n",
    "    plt.plot(xx, yy, 'k-')\n",
    "    plt.plot(xx, yy_down, 'k--')\n",
    "    plt.plot(xx, yy_up, 'k--')\n",
    "\n",
    "    plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=80,\n",
    "                facecolors='none', zorder=10)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=Y, zorder=10, cmap=plt.cm.Paired)\n",
    "\n",
    "    plt.axis('tight')\n",
    "    x_min = -4.8\n",
    "    x_max = 4.2\n",
    "    y_min = -6\n",
    "    y_max = 6\n",
    "\n",
    "    XX, YY = np.mgrid[x_min:x_max:200j, y_min:y_max:200j]\n",
    "    Z = clf.predict(np.c_[XX.ravel(), YY.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(XX.shape)\n",
    "    plt.figure(fignum, figsize=(4, 3))\n",
    "    plt.pcolormesh(XX, YY, Z, cmap=plt.cm.Paired)\n",
    "\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    fignum = fignum + 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Trick\n",
    "\n",
    "With the kernel trick we can obtain non-linear functions, as we show below for RBF and polynomial kernels. Example from http://scikit-learn.org/stable/auto_examples/svm/plot_iris.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(__doc__)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm, datasets\n",
    "\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # we only take the first two features. We could\n",
    "                      # avoid this ugly slicing by using a two-dim dataset\n",
    "y = iris.target\n",
    "\n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "# we create an instance of SVM and fit out data. We do not scale our\n",
    "# data since we want to plot the support vectors\n",
    "C = 1.0  # SVM regularization parameter\n",
    "svc = svm.SVC(kernel='linear', C=C).fit(X, y)\n",
    "rbf_svc = svm.SVC(kernel='rbf', gamma=0.7, C=C).fit(X, y)\n",
    "poly_svc = svm.SVC(kernel='poly', degree=3, C=C).fit(X, y)\n",
    "lin_svc = svm.LinearSVC(C=C).fit(X, y)\n",
    "\n",
    "# create a mesh to plot in\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "# title for the plots\n",
    "titles = ['SVC with linear kernel',\n",
    "          'LinearSVC (linear kernel)',\n",
    "          'SVC with RBF kernel',\n",
    "          'SVC with polynomial (degree 3) kernel']\n",
    "\n",
    "for i, clf in enumerate((svc, lin_svc, rbf_svc, poly_svc)):\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    plt.subplots_adjust(wspace=0.4, hspace=0.4)\n",
    "\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "\n",
    "    # Plot also the training points\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm)\n",
    "    plt.xlabel('Sepal length')\n",
    "    plt.ylabel('Sepal width')\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.title(titles[i])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effect of Sparsity on Runtime\n",
    "\n",
    "SVM has sparse solutions. This can sometimes have substantial effect on the runtime of the classifier and also of training.\n",
    "\n",
    "Support vector regression (SVR) uses the $\\epsilon$-insensitive loss function which does not penalize points within $\\epsilon$ of the true value and penalizes points linearly beyond that. The loss function can similarly produce sparse solutions.\n",
    "\n",
    "The example below (from http://scikit-learn.org/stable/auto_examples/plot_kernel_ridge_regression.html) shows the sparsity effect of SVR in comparison to kernal ridge regression which does not have sparse solution. Kernel ridge regression has a closed form solution so training may to be faster for small and medium sized datasets. For large training sets, the effect of sparsity in training becomes more important. SVR classifiers are usually faster than kernel ridge regression classifiers because of sparsity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Authors: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>\n",
    "# License: BSD 3 clause\n",
    "\n",
    "\n",
    "from __future__ import division\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rng = np.random.RandomState(0)\n",
    "\n",
    "X = 5 * rng.rand(10000, 1)\n",
    "y = np.sin(X).ravel()\n",
    "\n",
    "# Add noise to targets\n",
    "y[::5] += 3 * (0.5 - rng.rand(X.shape[0]/5))\n",
    "\n",
    "X_plot = np.linspace(0, 5, 100000)[:, None]\n",
    "\n",
    "train_size = 100\n",
    "svr = GridSearchCV(SVR(kernel='rbf', gamma=0.1), cv=5,\n",
    "                   param_grid={\"C\": [1e0, 1e1, 1e2, 1e3],\n",
    "                               \"gamma\": np.logspace(-2, 2, 5)})\n",
    "\n",
    "kr = GridSearchCV(KernelRidge(kernel='rbf', gamma=0.1), cv=5,\n",
    "                  param_grid={\"alpha\": [1e0, 0.1, 1e-2, 1e-3],\n",
    "                              \"gamma\": np.logspace(-2, 2, 5)})\n",
    "\n",
    "t0 = time.time()\n",
    "svr.fit(X[:train_size], y[:train_size])\n",
    "svr_fit = time.time() - t0\n",
    "print(\"SVR complexity and bandwidth selected and model fitted in %.3f s\"\n",
    "      % svr_fit)\n",
    "\n",
    "t0 = time.time()\n",
    "kr.fit(X[:train_size], y[:train_size])\n",
    "kr_fit = time.time() - t0\n",
    "print(\"KRR complexity and bandwidth selected and model fitted in %.3f s\"\n",
    "      % kr_fit)\n",
    "\n",
    "sv_ratio = svr.best_estimator_.support_.shape[0] / train_size\n",
    "print(\"Support vector ratio: %.3f\" % sv_ratio)\n",
    "\n",
    "t0 = time.time()\n",
    "y_svr = svr.predict(X_plot)\n",
    "svr_predict = time.time() - t0\n",
    "print(\"SVR prediction for %d inputs in %.3f s\"\n",
    "      % (X_plot.shape[0], svr_predict))\n",
    "\n",
    "t0 = time.time()\n",
    "y_kr = kr.predict(X_plot)\n",
    "kr_predict = time.time() - t0\n",
    "print(\"KRR prediction for %d inputs in %.3f s\"\n",
    "      % (X_plot.shape[0], kr_predict))\n",
    "sv_ind = svr.best_estimator_.support_\n",
    "plt.scatter(X[sv_ind], y[sv_ind], c='r', s=50, label='SVR support vectors',\n",
    "            zorder=2)\n",
    "plt.scatter(X[:100], y[:100], c='k', label='data', zorder=1)\n",
    "plt.hold('on')\n",
    "plt.plot(X_plot, y_svr, c='r',\n",
    "         label='SVR (fit: %.3fs, predict: %.3fs)' % (svr_fit, svr_predict))\n",
    "plt.plot(X_plot, y_kr, c='g',\n",
    "         label='KRR (fit: %.3fs, predict: %.3fs)' % (kr_fit, kr_predict))\n",
    "plt.xlabel('data')\n",
    "plt.ylabel('target')\n",
    "plt.title('SVR versus Kernel Ridge')\n",
    "plt.legend()\n",
    "\n",
    "# Visualize training and prediction time\n",
    "plt.figure()\n",
    "\n",
    "# Generate sample data\n",
    "X = 5 * rng.rand(10000, 1)\n",
    "y = np.sin(X).ravel()\n",
    "y[::5] += 3 * (0.5 - rng.rand(X.shape[0]/5))\n",
    "sizes = np.logspace(1, 4, 7)\n",
    "for name, estimator in {\"KRR\": KernelRidge(kernel='rbf', alpha=0.1,\n",
    "                                           gamma=10),\n",
    "                        \"SVR\": SVR(kernel='rbf', C=1e1, gamma=10)}.items():\n",
    "    train_time = []\n",
    "    test_time = []\n",
    "    for train_test_size in sizes:\n",
    "        t0 = time.time()\n",
    "        estimator.fit(X[:train_test_size], y[:train_test_size])\n",
    "        train_time.append(time.time() - t0)\n",
    "\n",
    "        t0 = time.time()\n",
    "        estimator.predict(X_plot[:1000])\n",
    "        test_time.append(time.time() - t0)\n",
    "\n",
    "    plt.plot(sizes, train_time, 'o-', color=\"r\" if name == \"SVR\" else \"g\",\n",
    "             label=\"%s (train)\" % name)\n",
    "    plt.plot(sizes, test_time, 'o--', color=\"r\" if name == \"SVR\" else \"g\",\n",
    "             label=\"%s (test)\" % name)\n",
    "\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Train size\")\n",
    "plt.ylabel(\"Time (seconds)\")\n",
    "plt.title('Execution Time')\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "# Visualize learning curves\n",
    "plt.figure()\n",
    "\n",
    "svr = SVR(kernel='rbf', C=1e1, gamma=0.1)\n",
    "kr = KernelRidge(kernel='rbf', alpha=0.1, gamma=0.1)\n",
    "train_sizes, train_scores_svr, test_scores_svr = \\\n",
    "    learning_curve(svr, X[:100], y[:100], train_sizes=np.linspace(0.1, 1, 10),\n",
    "                   scoring=\"neg_mean_squared_error\", cv=10)\n",
    "train_sizes_abs, train_scores_kr, test_scores_kr = \\\n",
    "    learning_curve(kr, X[:100], y[:100], train_sizes=np.linspace(0.1, 1, 10),\n",
    "                   scoring=\"neg_mean_squared_error\", cv=10)\n",
    "\n",
    "plt.plot(train_sizes, -test_scores_svr.mean(1), 'o-', color=\"r\",\n",
    "         label=\"SVR\")\n",
    "plt.plot(train_sizes, -test_scores_kr.mean(1), 'o-', color=\"g\",\n",
    "         label=\"KRR\")\n",
    "plt.xlabel(\"Train size\")\n",
    "plt.ylabel(\"Mean Squared Error\")\n",
    "plt.title('Learning curves')\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Prior Information in the Kernel\n",
    "\n",
    "The kernel can be used to encode prior information. To do that, you can use what you know about the problem to design the features, or your intuition of what the kernel as a similarity function should do. The following example weights one of the dimension much more heavily compared to the other. Example from  http://scikit-learn.org/stable/auto_examples/svm/plot_custom_kernel.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(__doc__)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm, datasets\n",
    "\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # we only take the first two features. We could\n",
    "                      # avoid this ugly slicing by using a two-dim dataset\n",
    "Y = iris.target\n",
    "\n",
    "\n",
    "def my_kernel(X, Y):\n",
    "    \"\"\"\n",
    "    We create a custom kernel:\n",
    "\n",
    "                 (10   0)\n",
    "    k(X, Y) = X  (      ) Y.T\n",
    "                 (0  0.1)\n",
    "    \"\"\"\n",
    "    M = np.array([[10, 0], [0, 0.1]])\n",
    "    return np.dot(np.dot(X, M), Y.T)\n",
    "\n",
    "\n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "# we create an instance of SVM and fit out data.\n",
    "clf = svm.SVC(kernel=my_kernel)\n",
    "clf.fit(X, Y)\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "# point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "\n",
    "# Plot also the training points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=Y, cmap=plt.cm.Paired)\n",
    "plt.title('3-Class classification using Support Vector Machine with custom'\n",
    "          ' kernel')\n",
    "plt.axis('tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
