{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimation II\n",
    "\n",
    "## Lasso\n",
    "The following example shows the good performance of Lasso when the weight of the linear function is sparse. The example shows simulation of data that may be obtained in computed tomography (CT).\n",
    "\n",
    "In the example, we have a sparse image showing the boundaries of objects. Each measurement is a projection -- inner product of another function (input image) with the sparse image (weight vector). In this case, we would like to recover the original sparse image, rather than to minimize prediction error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(__doc__)\n",
    "\n",
    "# Author: Emmanuelle Gouillart <emmanuelle.gouillart@nsup.org>\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from scipy import ndimage\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def _weights(x, dx=1, orig=0):\n",
    "    x = np.ravel(x)\n",
    "    floor_x = np.floor((x - orig) / dx)\n",
    "    alpha = (x - orig - floor_x * dx) / dx\n",
    "    return np.hstack((floor_x, floor_x + 1)), np.hstack((1 - alpha, alpha))\n",
    "\n",
    "\n",
    "def _generate_center_coordinates(l_x):\n",
    "    X, Y = np.mgrid[:l_x, :l_x].astype(np.float64)\n",
    "    center = l_x / 2.\n",
    "    X += 0.5 - center\n",
    "    Y += 0.5 - center\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def build_projection_operator(l_x, n_dir):\n",
    "    \"\"\" Compute the tomography design matrix.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    l_x : int\n",
    "        linear size of image array\n",
    "\n",
    "    n_dir : int\n",
    "        number of angles at which projections are acquired.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    p : sparse matrix of shape (n_dir l_x, l_x**2)\n",
    "    \"\"\"\n",
    "    X, Y = _generate_center_coordinates(l_x)\n",
    "    angles = np.linspace(0, np.pi, n_dir, endpoint=False)\n",
    "    data_inds, weights, camera_inds = [], [], []\n",
    "    data_unravel_indices = np.arange(l_x ** 2)\n",
    "    data_unravel_indices = np.hstack((data_unravel_indices,\n",
    "                                      data_unravel_indices))\n",
    "    for i, angle in enumerate(angles):\n",
    "        Xrot = np.cos(angle) * X - np.sin(angle) * Y\n",
    "        inds, w = _weights(Xrot, dx=1, orig=X.min())\n",
    "        mask = np.logical_and(inds >= 0, inds < l_x)\n",
    "        weights += list(w[mask])\n",
    "        camera_inds += list(inds[mask] + i * l_x)\n",
    "        data_inds += list(data_unravel_indices[mask])\n",
    "    proj_operator = sparse.coo_matrix((weights, (camera_inds, data_inds)))\n",
    "    return proj_operator\n",
    "\n",
    "\n",
    "def generate_synthetic_data():\n",
    "    \"\"\" Synthetic binary data \"\"\"\n",
    "    rs = np.random.RandomState(0)\n",
    "    n_pts = 36.\n",
    "    x, y = np.ogrid[0:l, 0:l]\n",
    "    mask_outer = (x - l / 2) ** 2 + (y - l / 2) ** 2 < (l / 2) ** 2\n",
    "    mask = np.zeros((l, l))\n",
    "    points = l * rs.rand(2, n_pts)\n",
    "    mask[(points[0]).astype(np.int), (points[1]).astype(np.int)] = 1\n",
    "    mask = ndimage.gaussian_filter(mask, sigma=l / n_pts)\n",
    "    res = np.logical_and(mask > mask.mean(), mask_outer)\n",
    "    return res - ndimage.binary_erosion(res)\n",
    "\n",
    "\n",
    "# Generate synthetic images, and projections\n",
    "l = 128\n",
    "proj_operator = build_projection_operator(l, l / 7.)\n",
    "data = generate_synthetic_data()\n",
    "proj = proj_operator * data.ravel()[:, np.newaxis]\n",
    "proj += 0.15 * np.random.randn(*proj.shape)\n",
    "\n",
    "# Reconstruction with L2 (Ridge) penalization\n",
    "rgr_ridge = Ridge(alpha=0.2)\n",
    "rgr_ridge.fit(proj_operator, proj.ravel())\n",
    "rec_l2 = rgr_ridge.coef_.reshape(l, l)\n",
    "\n",
    "# Reconstruction with L1 (Lasso) penalization\n",
    "# the best value of alpha was determined using cross validation\n",
    "# with LassoCV\n",
    "rgr_lasso = Lasso(alpha=0.001)\n",
    "rgr_lasso.fit(proj_operator, proj.ravel())\n",
    "rec_l1 = rgr_lasso.coef_.reshape(l, l)\n",
    "\n",
    "plt.figure(figsize=(8, 3.3))\n",
    "plt.subplot(131)\n",
    "plt.imshow(data, cmap=plt.cm.gray, interpolation='nearest')\n",
    "plt.axis('off')\n",
    "plt.title('original image')\n",
    "plt.subplot(132)\n",
    "plt.imshow(rec_l2, cmap=plt.cm.gray, interpolation='nearest')\n",
    "plt.title('L2 penalization')\n",
    "plt.axis('off')\n",
    "plt.subplot(133)\n",
    "plt.imshow(rec_l1, cmap=plt.cm.gray, interpolation='nearest')\n",
    "plt.title('L1 penalization')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplots_adjust(hspace=0.01, wspace=0.01, top=1, bottom=0, left=0,\n",
    "                    right=1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text classification with $l_1$ and $l_2$ regularized logistic regression\n",
    "Modified from http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline  \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "np.random.seed(1)\n",
    "# Select only 4 categories to speed things up\n",
    "categories = ['alt.atheism', 'soc.religion.christian','comp.graphics', 'sci.med']\n",
    "\n",
    "# Fetch training and test sets\n",
    "twenty_train = fetch_20newsgroups(subset='train',\n",
    "                                  categories=categories, shuffle=True, random_state=42)\n",
    "twenty_test = fetch_20newsgroups(subset='test',\n",
    "                                 categories=categories, shuffle=True, random_state=42)\n",
    "\n",
    "for i, C in enumerate((100000, 10000, 1000, 100)):\n",
    "    LR1 = Pipeline([('vect', CountVectorizer()),\n",
    "                   ('tfidf', TfidfTransformer()),\n",
    "                   ('clf1', LogisticRegression(C=C, penalty='l1')),])\n",
    "\n",
    "    LR1.fit(twenty_train.data, twenty_train.target)\n",
    "    LR1Predicted = LR1.predict(twenty_test.data)\n",
    "    coef_l1_LR = LR1.named_steps['clf1'].coef_.ravel()\n",
    "\n",
    "    # coef_l1_LR contains zeros due to the\n",
    "    # L1 sparsity inducing norm\n",
    "\n",
    "    sparsity_l1_LR = np.mean(coef_l1_LR == 0) * 100\n",
    "    print \"l1: C: \" + repr(C) + \"   Error: \" + repr(np.mean(LR1Predicted == twenty_test.target)) + \" Sparsity: \" + repr(sparsity_l1_LR)\n",
    "\n",
    "for i, C in enumerate((100000, 10000, 1000, 100)):\n",
    "    LR2 = Pipeline([('vect', CountVectorizer()),\n",
    "                   ('tfidf', TfidfTransformer()),\n",
    "                   ('clf2', LogisticRegression(C=C, penalty='l2')),])\n",
    "\n",
    "    LR2.fit(twenty_train.data, twenty_train.target)\n",
    "    LR2Predicted = LR2.predict(twenty_test.data)\n",
    "    coef_l2_LR = LR2.named_steps['clf2'].coef_.ravel()\n",
    "    sparsity_l2_LR = np.mean(coef_l2_LR == 0) * 100\n",
    "    print \"l2: C: \" + repr(C) + \"   Error: \" + repr(np.mean(LR2Predicted == twenty_test.target)) + \" Sparsity: \" + repr(sparsity_l2_LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias and Variance for Bagging\n",
    "\n",
    "The following example measure the bias and variance of bagging decision tree regressors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(__doc__)\n",
    "\n",
    "# Author: Gilles Louppe <g.louppe@gmail.com>\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Settings\n",
    "n_repeat = 50       # Number of iterations for computing expectations\n",
    "n_train = 50        # Size of the training set\n",
    "n_test = 1000       # Size of the test set\n",
    "noise = 0.1         # Standard deviation of the noise\n",
    "np.random.seed(0)\n",
    "\n",
    "# Change this for exploring the bias-variance decomposition of other\n",
    "# estimators. This should work well for estimators with high variance (e.g.,\n",
    "# decision trees or KNN), but poorly for estimators with low variance (e.g.,\n",
    "# linear models).\n",
    "estimators = [(\"Tree\", DecisionTreeRegressor()),\n",
    "              (\"Bagging(Tree)\", BaggingRegressor(DecisionTreeRegressor()))]\n",
    "\n",
    "n_estimators = len(estimators)\n",
    "\n",
    "# Generate data\n",
    "def f(x):\n",
    "    x = x.ravel()\n",
    "\n",
    "    return np.exp(-x ** 2) + 1.5 * np.exp(-(x - 2) ** 2)\n",
    "\n",
    "def generate(n_samples, noise, n_repeat=1):\n",
    "    X = np.random.rand(n_samples) * 10 - 5\n",
    "    X = np.sort(X)\n",
    "\n",
    "    if n_repeat == 1:\n",
    "        y = f(X) + np.random.normal(0.0, noise, n_samples)\n",
    "    else:\n",
    "        y = np.zeros((n_samples, n_repeat))\n",
    "\n",
    "        for i in range(n_repeat):\n",
    "            y[:, i] = f(X) + np.random.normal(0.0, noise, n_samples)\n",
    "\n",
    "    X = X.reshape((n_samples, 1))\n",
    "\n",
    "    return X, y\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for i in range(n_repeat):\n",
    "    X, y = generate(n_samples=n_train, noise=noise)\n",
    "    X_train.append(X)\n",
    "    y_train.append(y)\n",
    "\n",
    "X_test, y_test = generate(n_samples=n_test, noise=noise, n_repeat=n_repeat)\n",
    "\n",
    "# Loop over estimators to compare\n",
    "for n, (name, estimator) in enumerate(estimators):\n",
    "    # Compute predictions\n",
    "    y_predict = np.zeros((n_test, n_repeat))\n",
    "\n",
    "    for i in range(n_repeat):\n",
    "        estimator.fit(X_train[i], y_train[i])\n",
    "        y_predict[:, i] = estimator.predict(X_test)\n",
    "\n",
    "    # Bias^2 + Variance + Noise decomposition of the mean squared error\n",
    "    y_error = np.zeros(n_test)\n",
    "\n",
    "    for i in range(n_repeat):\n",
    "        for j in range(n_repeat):\n",
    "            y_error += (y_test[:, j] - y_predict[:, i]) ** 2\n",
    "\n",
    "    y_error /= (n_repeat * n_repeat)\n",
    "\n",
    "    y_noise = np.var(y_test, axis=1)\n",
    "    y_bias = (f(X_test) - np.mean(y_predict, axis=1)) ** 2\n",
    "    y_var = np.var(y_predict, axis=1)\n",
    "\n",
    "    print(\"{0}: {1:.4f} (error) = {2:.4f} (bias^2) \"\n",
    "          \" + {3:.4f} (var) + {4:.4f} (noise)\".format(name,\n",
    "                                                      np.mean(y_error),\n",
    "                                                      np.mean(y_bias),\n",
    "                                                      np.mean(y_var),\n",
    "                                                      np.mean(y_noise)))\n",
    "    plt.figure(1, figsize=(12, 9))\n",
    "    # Plot figures\n",
    "    plt.subplot(2, n_estimators, n + 1)\n",
    "    plt.plot(X_test, f(X_test), \"b\", label=\"$f(x)$\")\n",
    "    plt.plot(X_train[0], y_train[0], \".b\", label=\"LS ~ $y = f(x)+noise$\")\n",
    "\n",
    "    for i in range(n_repeat):\n",
    "        if i == 0:\n",
    "            plt.plot(X_test, y_predict[:, i], \"r\", label=\"$\\^y(x)$\")\n",
    "        else:\n",
    "            plt.plot(X_test, y_predict[:, i], \"r\", alpha=0.05)\n",
    "\n",
    "    plt.plot(X_test, np.mean(y_predict, axis=1), \"c\",\n",
    "             label=\"$\\mathbb{E}_{LS} \\^y(x)$\")\n",
    "\n",
    "    plt.xlim([-5, 5])\n",
    "    plt.title(name)\n",
    "\n",
    "    if n == 0:\n",
    "        plt.legend(loc=\"upper left\", prop={\"size\": 11})\n",
    "\n",
    "    plt.subplot(2, n_estimators, n_estimators + n + 1)\n",
    "    plt.plot(X_test, y_error, \"r\", label=\"$error(x)$\")\n",
    "    plt.plot(X_test, y_bias, \"b\", label=\"$bias^2(x)$\"),\n",
    "    plt.plot(X_test, y_var, \"g\", label=\"$variance(x)$\"),\n",
    "    plt.plot(X_test, y_noise, \"c\", label=\"$noise(x)$\")\n",
    "\n",
    "    plt.xlim([-5, 5])\n",
    "    plt.ylim([0, 0.1])\n",
    "\n",
    "    if n == 0:\n",
    "        plt.legend(loc=\"upper left\", prop={\"size\": 11})\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
