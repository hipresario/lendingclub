{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Exercises for Formulation Lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, a high degree polynomial is able to fit the training data perfectly, but has much worst test error compared to a linear function. The training and test examples are generated with the function $y=x$ with Gaussian noise added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Overfitting example.\n",
    "# Noisy training and test set generated using a linear function\n",
    "# Overfit when learning with high degree polynomial\n",
    "\n",
    "%matplotlib inline  \n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Set seed for random number generator to make results reproducible\n",
    "np.random.seed(1) \n",
    "\n",
    "# Number of data points in train and test sets\n",
    "data_size = 100\n",
    "data_interval = 10.0/data_size\n",
    "\n",
    "# Linear regression model\n",
    "linear = LinearRegression(fit_intercept=True,normalize=True)\n",
    "# Polynomial regression model, degree the same as data size\n",
    "poly = Pipeline([('poly', PolynomialFeatures(degree=data_size)),\n",
    "                 ('lin', LinearRegression(fit_intercept=True, normalize=True))])\n",
    "\n",
    "# Construct training set\n",
    "# Odd number between 1 to 10 as training inputs\n",
    "# Output is y = x + noise\n",
    "xtrain = np.arange(data_interval/2, 10, data_interval)\n",
    "train_noise = np.random.normal(0, 1, data_size)\n",
    "ytrain = xtrain + train_noise\n",
    "\n",
    "# Fit the models\n",
    "linear = linear.fit(xtrain[:, np.newaxis], ytrain)\n",
    "poly = poly.fit(xtrain[:, np.newaxis], ytrain)\n",
    "\n",
    "# Construct test set\n",
    "# Even numbers between 1 to 10 as test inputs\n",
    "xtest = np.arange(data_interval,10 + data_interval/2, data_interval)\n",
    "test_noise = np.random.normal(0, 1, data_size)\n",
    "\n",
    "# Do predictions\n",
    "linear_pred = linear.predict(xtest[:,np.newaxis])\n",
    "poly_pred = poly.predict(xtest[:,np.newaxis])\n",
    "\n",
    "# Measure mean squared error\n",
    "ytest = xtest + test_noise\n",
    "linerror = mean_squared_error(ytest, linear_pred)\n",
    "polyerror = mean_squared_error(ytest, poly_pred)\n",
    "\n",
    "# Plotting\n",
    "x_plot = np.linspace(0, 10, 100)\n",
    "\n",
    "fig = plt.figure(1, figsize=(12, 9))\n",
    "fig.clf()\n",
    "\n",
    "sub1 = fig.add_subplot(2,2,1)\n",
    "sub1.set_title('Lin Reg Train Set')\n",
    "sub1.scatter(xtrain, ytrain,  color='red')\n",
    "sub1.plot(x_plot, linear.predict(x_plot[:,np.newaxis]), color='green',linewidth=3)\n",
    "\n",
    "sub2 = fig.add_subplot(2,2,2)\n",
    "sub2.set_title('Lin Reg Test Set')\n",
    "sub2.scatter(xtest, ytest,  color='red')\n",
    "sub2.plot(x_plot, linear.predict(x_plot[:,np.newaxis]), color='green',linewidth=3)\n",
    "\n",
    "sub3 = fig.add_subplot(2,2,3)\n",
    "sub3.set_title('Poly Reg Train Set')\n",
    "sub3.scatter(xtrain, ytrain,  color='red')\n",
    "sub3.plot(x_plot, poly.predict(x_plot[:,np.newaxis]), color='green',linewidth=3)\n",
    "\n",
    "sub4 = fig.add_subplot(2,2,4)\n",
    "sub4.set_title('Poly Reg Test Set')\n",
    "sub4.scatter(xtest, ytest,  color='red')\n",
    "sub4.plot(x_plot, poly.predict(x_plot[:,np.newaxis]), color='green',linewidth=3)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Linear test set error: \" + \"{0:.2f}\".format(linerror)\n",
    "print \"Poly test set error: \" + \"{0:.2f}\".format(polyerror)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "Regularization is one method for combatting overfitting. Regularized linear regression is often called ridge regression. In the following, we try to reduce overfitting for linear regression using polynomial features by doing ridge regression. In scikit learn, ridge regression finds $\\min_w ||Xw - y||_2^2 +\\alpha||w||_2^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Polynomial regression model with ridge regression, degree the same as data size\n",
    "ridgepoly = Pipeline([('poly', PolynomialFeatures(degree=data_size)),\n",
    "                      ('ridgereg', Ridge(alpha = 0.01, fit_intercept=True,normalize=True))])\n",
    "ridgepoly = ridgepoly.fit(xtrain[:, np.newaxis], ytrain)\n",
    "ridgepoly_pred = ridgepoly.predict(xtest[:,np.newaxis])\n",
    "ridgepolyerror = mean_squared_error(ytest, ridgepoly_pred)\n",
    "\n",
    "fig = plt.figure(1, figsize=(12, 4.5))\n",
    "fig.clf()\n",
    "\n",
    "sub1 = fig.add_subplot(1,2,1)\n",
    "sub1.set_title('Poly Reg Test Set')\n",
    "sub1.scatter(xtest, ytest,  color='red')\n",
    "sub1.plot(x_plot, poly.predict(x_plot[:,np.newaxis]), color='green',linewidth=3)\n",
    "\n",
    "sub2 = fig.add_subplot(1,2,2)\n",
    "sub2.set_title('Ridge Poly Reg Test Set')\n",
    "sub2.scatter(xtest, ytest,  color='red')\n",
    "sub2.plot(x_plot, ridgepoly.predict(x_plot[:,np.newaxis]), color='green',linewidth=3)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print \"Ridge poly test set error: \" + \"{0:.2f}\".format(ridgepolyerror)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be careful if the test set is quite different from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Prediction at x = 15 is \" + \"{0:.2e}\".format(ridgepoly.predict([[15]])[0]) + \" when the true value is 15.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection: Sparsity Inducing Regularizer\n",
    "Regularizing using the $\\ell_1$ norm rather than the $\\ell_2$ norm induces sparsity and can serve as a method for feature selection. This is often called Lasso. The optimization objective for Lasso is $\\frac{1}{2m}||y - Xw||^2_2 + \\alpha ||w||_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Polynomial regression model with Lasso, degree the same as data set size\n",
    "lassopoly = Pipeline([('poly', PolynomialFeatures(degree=data_size)),\n",
    "                      ('lassoreg', Lasso(alpha = 0.005, fit_intercept=True,normalize=True))])\n",
    "lassopoly = lassopoly.fit(xtrain[:, np.newaxis], ytrain)\n",
    "lassopoly_pred = lassopoly.predict(xtest[:,np.newaxis])\n",
    "lassopolyerror = mean_squared_error(ytest, lassopoly_pred)\n",
    "\n",
    "fig = plt.figure(1, figsize=(12, 4.5))\n",
    "fig.clf()\n",
    "\n",
    "sub1 = fig.add_subplot(1,2,1)\n",
    "sub1.set_title('Ridge Poly Reg Test Set')\n",
    "sub1.scatter(xtest, ytest,  color='red')\n",
    "sub1.plot(x_plot, ridgepoly.predict(x_plot[:,np.newaxis]), color='green',linewidth=3)\n",
    "\n",
    "sub2 = fig.add_subplot(1,2,2)\n",
    "sub2.set_title('Lasso Poly Reg Test Set')\n",
    "sub2.scatter(xtest, ytest,  color='red')\n",
    "sub2.plot(x_plot, lassopoly.predict(x_plot[:,np.newaxis]), color='green',linewidth=3)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Lasso poly test set error: \" + \"{0:.2f}\".format(lassopolyerror)\n",
    "print \"Lasso coeffs: \" + repr(lassopoly.named_steps['lassoreg'].coef_)\n",
    "print \"Ridge coeffs: \" + repr(ridgepoly.named_steps['ridgereg'].coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Transformation and Normalization\n",
    "Feature transformation and normalization can affect performance of some learning methods. Consider the same problem learned using ridge regression and lasso without normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unnormalizedridge = Pipeline([('poly', PolynomialFeatures(degree=data_size)),\n",
    "                      ('ridgereg', Ridge(alpha = 0.01, fit_intercept=True,normalize=False))])\n",
    "unnormalizedridge = unnormalizedridge.fit(xtrain[:, np.newaxis], ytrain)\n",
    "unnormalizedridgepoly_pred = unnormalizedridge.predict(xtest[:,np.newaxis])\n",
    "\n",
    "unnormalizedlasso = Pipeline([('poly', PolynomialFeatures(degree=data_size)),\n",
    "                              ('lassoreg',Lasso(alpha = 0.005, fit_intercept=True,\n",
    "                                                normalize=False))])\n",
    "unnormalizedlasso = unnormalizedlasso.fit(xtrain[:, np.newaxis], ytrain)\n",
    "unnormalizedlasso_pred = unnormalizedlasso.predict(xtest[:,np.newaxis])\n",
    "\n",
    "fig = plt.figure(1, figsize=(12, 4.5))\n",
    "fig.clf()\n",
    "\n",
    "sub1 = fig.add_subplot(1,2,1)\n",
    "sub1.set_title('Unnormalized Ridge Reg Test Set')\n",
    "sub1.scatter(xtest, ytest,  color='red')\n",
    "sub1.plot(x_plot, unnormalizedridge.predict(x_plot[:,np.newaxis]), color='green',linewidth=3)\n",
    "\n",
    "sub2 = fig.add_subplot(1,2,2)\n",
    "sub2.set_title('Unnormalized Lasso Reg Test Set')\n",
    "sub2.scatter(xtest, ytest,  color='red')\n",
    "sub2.plot(x_plot, unnormalizedlasso.predict(x_plot[:,np.newaxis]), color='green',linewidth=3)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "In the following example, we put together model selection using cross validation for selecting the regularization parameter for regularized logistic regression and the number of components of PCA to use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Modified from http://scikit-learn.org/stable/tutorial/statistical_inference/putting_together.html\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import linear_model, decomposition, datasets\n",
    "\n",
    "logistic = linear_model.LogisticRegression()\n",
    "\n",
    "pca = decomposition.PCA()\n",
    "pipe = Pipeline(steps=[('pca', pca), ('logistic', logistic)])\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# Show the images\n",
    "images_and_labels = list(zip(digits.images, digits.target))\n",
    "for index, (image, label) in enumerate(images_and_labels[:4]):\n",
    "    plt.subplot(1, 4, index + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.title('Training: %i' % label)\n",
    "plt.show()\n",
    "\n",
    "X_digits = digits.data\n",
    "y_digits = digits.target\n",
    "\n",
    "###############################################################################\n",
    "# Plot the PCA spectrum\n",
    "pca.fit(X_digits)\n",
    "\n",
    "plt.figure(6, figsize=(4, 3))\n",
    "plt.clf()\n",
    "plt.axes([.2, .2, .7, .7])\n",
    "plt.plot(pca.explained_variance_, linewidth=2)\n",
    "plt.axis('tight')\n",
    "plt.xlabel('n_components')\n",
    "plt.ylabel('explained_variance_')\n",
    "\n",
    "###############################################################################\n",
    "# Prediction\n",
    "\n",
    "n_components = [20, 40, 64]\n",
    "Cs = np.logspace(-3, 0, 3)\n",
    "\n",
    "#Parameters of pipelines can be set using ‘__’ separated parameter names:\n",
    "\n",
    "estimator = GridSearchCV(pipe,\n",
    "                         dict(pca__n_components=n_components,\n",
    "                              logistic__C=Cs))\n",
    "estimator.fit(X_digits, y_digits)\n",
    "\n",
    "plt.axvline(estimator.best_estimator_.named_steps['pca'].n_components,\n",
    "            linestyle=':', label='n_components chosen')\n",
    "plt.legend(prop=dict(size=12))\n",
    "plt.show()\n",
    "estimator.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminative vs Generative\n",
    "In this section, we compare learning with discriminative and generative models. We will use linear discriminant analysis as the generative model and logistic regression as the discriminative model.\n",
    "\n",
    "For details on scikit learn's\n",
    "* Linear Discriminant Analysis, see http://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.htmlFor \n",
    "* Logistic regression, see http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression.\n",
    "\n",
    "For the first example, we plot the learning curves for data generated from two isotropic Gaussians with centers $(-1, -1, -1, -1), (1, 1, 1, 1)$ and standard deviations of 1. In this case, both models are able to represent the optimal decision boundary and we would like to see which method converges faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Code modified from http://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=1, train_sizes=np.linspace(.05, 1.0, 10)):\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n",
    "\n",
    "n_samples = 1000    \n",
    "# Generate samples from two Gaussians centers\n",
    "centers = [(-1, -1, -1, -1), (1, 1, 1, 1)]\n",
    "X, y = make_blobs(n_samples=n_samples, n_features=4, cluster_std=1.0,\n",
    "                  centers=centers, shuffle=True, random_state=1)\n",
    "\n",
    "title = \"Learning Curves (LDA)\"\n",
    "# Cross validation with 100 iterations to get smoother mean test and train\n",
    "# score curves, each time with 20% data randomly selected as a validation set.\n",
    "cv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=0)\n",
    "\n",
    "estimator = LinearDiscriminantAnalysis()\n",
    "plot_learning_curve(estimator, title, X, y, ylim=(0.7, 1.01), cv=cv, n_jobs=4)\n",
    "\n",
    "title = \"Learning Curves (Logistic Regression)\"\n",
    "cv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=0)\n",
    "estimator = LogisticRegression(C=1000) # large C to avoid regularization\n",
    "plot_learning_curve(estimator, title, X, y, (0.7, 1.01), cv=cv, n_jobs=4)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now look at a handwritten digit classification problem. In this case, the true model is unknown. We regularize to try to get reasonable convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "\n",
    "title = \"Learning Curves (LDA)\"\n",
    "# Cross validation with 10 iterations\n",
    "# score curves, each time with 20% data randomly selected as a validation set.\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
    "estimator = LinearDiscriminantAnalysis(solver='eigen', shrinkage=0.4)\n",
    "plot_learning_curve(estimator, title, X, y, ylim=(0.7, 1.01), cv=cv, n_jobs=4)\n",
    "\n",
    "title = \"Learning Curves (Logistic Regression)\"\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
    "estimator = LogisticRegression(C=0.005, multi_class='multinomial', solver='newton-cg')\n",
    "plot_learning_curve(estimator, title, X, y, (0.7, 1.01), cv=cv, n_jobs=4)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
